{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353ad50e-62f6-4148-823f-edf531bfcbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48faa04-56b4-41da-a53d-d7e457d13a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m nltk.downloader punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b796ff79-304f-4d05-b6ab-6cdaa1a32d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import tiktoken\n",
    "\n",
    "encoder = tiktoken.get_encoding('cl100k_base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47863822-c3db-4777-af97-41d23184d1e6",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "# Load dataset data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a257db6-8c21-431f-b197-57218b20349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_data_path = Path('ST.json')\n",
    "with st_data_path.open('r') as f:\n",
    "    st_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569757ef-d0ed-432e-b08d-ac7f68e3f346",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "# Get metrics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20a64512-8854-46e2-af72-2f0eb1cfea28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_dict(data):\n",
    "    metrics_data = dict()\n",
    "    for doc_id, doc_data in enumerate(data):        \n",
    "        doc_metrics = dict()\n",
    "\n",
    "        # --- Text centered stats\n",
    "        doc_metrics['text'] = {\n",
    "            'sentence count': len(sent_tokenize(doc_data['doc']['text'])),\n",
    "            'token count': len(encoder.encode(doc_data['doc']['text']))\n",
    "        }\n",
    "\n",
    "        # --- Entity centered metrics\n",
    "        gpt_entities = doc_data['entities']['gpt']\n",
    "        gpt_correct_entities = [e for e in gpt_entities if e['annotation']['entity correctness']]\n",
    "        gpt_missed_entities = doc_data['entities']['annotation']['gpt missed']\n",
    "        gpt_entity_description_from_text = [\n",
    "            e for e in gpt_entities\n",
    "            if e['annotation']['entity correctness'] and e['annotation']['description from text']\n",
    "        ]\n",
    "        doc_metrics['entities'] = {\n",
    "            'total':         len(gpt_entities),\n",
    "            'correct':       len(gpt_correct_entities),\n",
    "            'missing':       len(gpt_missed_entities),\n",
    "            'from text':     len(gpt_entity_description_from_text),\n",
    "        }\n",
    "    \n",
    "        # --- Type centered metrics\n",
    "        gpt_types_all = [t for e in gpt_entities for t in e['types']]\n",
    "        gpt_types_first = [e['types'][0] for e in gpt_entities]\n",
    "        gpt_correct_types_all = [t for e in gpt_entities for t, b in zip(e['types'], e['annotation']['type correctness']) if b]\n",
    "        gpt_correct_types_first = [e['types'][0] for e in gpt_entities if e['annotation']['type correctness'][0]]\n",
    "        doc_metrics['types'] = {\n",
    "            'all total':   len(gpt_types_all),\n",
    "            'first total':   len(gpt_types_first),\n",
    "            'all correct': len(gpt_correct_types_all),\n",
    "            'first correct': len(gpt_correct_types_first),\n",
    "        }\n",
    "    \n",
    "        # --- Triple centered metrics\n",
    "        gpt_triples = doc_data['triples']['gpt']\n",
    "        gpt_correct_triples = [t for t in gpt_triples if t['annotation']['triple correctness']]\n",
    "        gpt_relation_from_text = [\n",
    "            t for t in gpt_triples\n",
    "            if t['annotation']['triple correctness'] and t['annotation']['relation from text']\n",
    "        ]\n",
    "        doc_metrics['triples'] = {\n",
    "            'total':         len(gpt_triples),\n",
    "            'correct':       len(gpt_correct_triples),\n",
    "            'from text':     len(gpt_relation_from_text),\n",
    "        }\n",
    "\n",
    "        # --- Track metrics\n",
    "        metrics_data[doc_id] = doc_metrics\n",
    "    \n",
    "    return metrics_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "439f1a26-6db1-4f6a-b716-56da316f05b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = get_metrics_dict(st_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9c1f2f-2ae4-4e1a-9946-afdb77ffe8b5",
   "metadata": {},
   "source": [
    "## Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3decc6c-edb9-428c-8553-2a9d158e1d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 44\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of documents: {len(st_data):,d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ae07035-97ae-46ec-97a1-d36819c7f52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entities: 761\n"
     ]
    }
   ],
   "source": [
    "ce = sum([doc['entities']['total'] for _, doc in m.items()])\n",
    "print(f'Number of entities: {ce:,d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79df9ba3-8196-4b57-b30c-75b1f267b566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of triples: 640\n"
     ]
    }
   ],
   "source": [
    "ct = sum([doc['triples']['total'] for _, doc in m.items()])\n",
    "print(f'Number of triples: {ct:,d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e59b205-44c1-4514-b151-9368b30e27cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence count: 888\n"
     ]
    }
   ],
   "source": [
    "sentence_count = sum([doc['text']['sentence count'] for _, doc in m.items()])\n",
    "print(f'Sentence count: {sentence_count:,d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6893f4cf-0621-499d-9b64-25f9d3e06b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average token count: 657\n"
     ]
    }
   ],
   "source": [
    "token_count_list = [doc['text']['token count'] for _, doc in m.items()]\n",
    "print(f'Average token count: {round(sum(token_count_list)/len(token_count_list)):,d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aeda45-8641-4f6e-bc76-07748b073bd9",
   "metadata": {},
   "source": [
    "## Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1554acae-11ea-41b2-91d7-28181a3bae25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro precision entities:  0.974\n"
     ]
    }
   ],
   "source": [
    "pe = sum([doc['entities']['correct'] for _, doc in m.items()]) / sum([doc['entities']['total'] for _, doc in m.items()])\n",
    "print(f'Micro precision entities: {pe: 4.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3127ca6b-cc4f-47ff-a339-b69c00e42395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro recall entities:  0.943\n"
     ]
    }
   ],
   "source": [
    "re = sum([doc['entities']['correct'] for _, doc in m.items()]) / sum([doc['entities']['correct'] + doc['entities']['missing'] for _, doc in m.items()])\n",
    "print(f'Micro recall entities: {re: 4.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3821b9b4-9a4c-43a1-98a8-a016e7d7850b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1 entities:  0.958\n"
     ]
    }
   ],
   "source": [
    "f1e = 2 / (1 / pe + 1 / re)\n",
    "print(f'Micro F1 entities: {f1e: 4.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e1e9068-ad59-4b92-94f5-645889adaf58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro precision types (all):  0.857\n"
     ]
    }
   ],
   "source": [
    "pta = sum([doc['types']['all correct'] for _, doc in m.items()]) / sum([doc['types']['all total'] for _, doc in m.items()])\n",
    "print(f'Micro precision types (all): {pta: 4.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64b0789a-4a30-407c-9544-96c3e25cd559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro precision types (only first):  0.951\n"
     ]
    }
   ],
   "source": [
    "ptf = sum([doc['types']['first correct'] for _, doc in m.items()]) / sum([doc['types']['first total'] for _, doc in m.items()])\n",
    "print(f'Micro precision types (only first): {ptf: 4.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eacb1116-dd5f-4e2c-9bcb-dad980d9606e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro precision triples:  0.753\n"
     ]
    }
   ],
   "source": [
    "pr = sum([doc['triples']['correct'] for _, doc in m.items()]) / sum([doc['triples']['total'] for _, doc in m.items()])\n",
    "print(f'Micro precision triples: {pr: 4.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51c3e333-ed16-4d68-91ad-a73e8d2beeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of entities descriptions from GPT knowledge:  0.094\n"
     ]
    }
   ],
   "source": [
    "ae = 1 - (sum([doc['entities']['from text'] for _, doc in m.items()]) / sum([doc['entities']['correct'] for _, doc in m.items()]))\n",
    "print(f'Percentage of entities descriptions from GPT knowledge: {ae: 4.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e0d5a51-f2ce-4ec5-a713-446b918b4f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of triples from GPT knowledge:  0.000\n"
     ]
    }
   ],
   "source": [
    "at = 1 - (sum([doc['triples']['from text'] for _, doc in m.items()]) / sum([doc['triples']['correct'] for _, doc in m.items()]))\n",
    "print(f'Percentage of triples from GPT knowledge: {at: 4.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af4f02e-e446-4684-a54c-8ddba2e9081c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
