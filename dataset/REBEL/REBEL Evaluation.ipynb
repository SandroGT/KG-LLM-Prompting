{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a32a78-8042-4b84-9487-9a310691ad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install nltk numpy scikit-learn sentence-transformers --no-input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1304b5-0dd1-4992-8e87-45f0f48f5ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m nltk.downloader punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b796ff79-304f-4d05-b6ab-6cdaa1a32d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, RegexpTokenizer\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import tiktoken\n",
    "\n",
    "encoder = tiktoken.get_encoding('cl100k_base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47863822-c3db-4777-af97-41d23184d1e6",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "# Load dataset data (with GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "296a31cb-2138-406b-a943-d7ec6f772a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "rebel_data_path = Path('REBEL.json')\n",
    "with rebel_data_path.open('r') as f:\n",
    "    rebel_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569757ef-d0ed-432e-b08d-ac7f68e3f346",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "# Get counts data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87673749-bf6b-475d-adce-7e375710e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts_dict(data):\n",
    "    counts_data = dict()\n",
    "    for doc_id, doc_data in enumerate(data):                \n",
    "        doc_counts = dict()\n",
    "\n",
    "        # --- Text centered stats\n",
    "        doc_counts['text'] = {\n",
    "            'sentence count': len(sent_tokenize(doc_data['doc']['text'])),\n",
    "            'token count': len(encoder.encode(doc_data['doc']['text']))\n",
    "        }\n",
    "\n",
    "        # --- Entity centered stats\n",
    "        gpt_entities = doc_data['entities']['gpt']\n",
    "        rebel_entities = list(set([e['surfaceform'] for e in doc_data['entities']['gold']]))\n",
    "        doc_counts['entities'] = {\n",
    "            'total':         len(gpt_entities),\n",
    "            'rebel count':   len(rebel_entities),\n",
    "        }\n",
    "        \n",
    "        # --- Triplet centered stats\n",
    "        gpt_triples = doc_data['triples']['gpt']\n",
    "        rebel_triples = list(set([\n",
    "            '|'.join([t[role]['surfaceform'].strip() for role in ['subject', 'predicate', 'object']])\n",
    "            for t in doc_data['triples']['gold']\n",
    "        ]))\n",
    "        doc_counts['triples'] = {\n",
    "            'total':         len(gpt_triples),\n",
    "            'rebel count':   len(rebel_triples),\n",
    "        }\n",
    "\n",
    "\n",
    "        # --- Track metrics\n",
    "        counts_data[doc_id] = doc_counts\n",
    "    \n",
    "    return counts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a2ffe6b-186c-46fa-94e0-3b25d430c3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = get_counts_dict(rebel_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9c1f2f-2ae4-4e1a-9946-afdb77ffe8b5",
   "metadata": {},
   "source": [
    "## Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b2ee149-83d2-43a5-b231-636ae7e7573d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 148\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of documents: {len(rebel_data):,d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f26d0d76-63b1-4017-a09b-fab230a03c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entities: 2,845\n"
     ]
    }
   ],
   "source": [
    "ce = sum([doc['entities']['total'] for _, doc in m.items()])\n",
    "print(f'Number of entities: {ce:,d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94068ff2-d553-4ad8-9cc9-60883c187630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of triples: 2,024\n"
     ]
    }
   ],
   "source": [
    "ct = sum([doc['triples']['total'] for _, doc in m.items()])\n",
    "print(f'Number of triples: {ct:,d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0974b301-9d9f-47d5-9e52-acaf286231f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence count: 2,803\n"
     ]
    }
   ],
   "source": [
    "sentence_count = sum([doc['text']['sentence count'] for _, doc in m.items()])\n",
    "print(f'Sentence count: {sentence_count:,d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52a74683-ad30-487c-82df-69a74bac0324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average token count: 605\n"
     ]
    }
   ],
   "source": [
    "token_count_list = [doc['text']['token count'] for _, doc in m.items()]\n",
    "print(f'Average token count: {round(sum(token_count_list)/len(token_count_list)):,d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8e0d08-1df0-45b9-9286-0540958ef86f",
   "metadata": {},
   "source": [
    "# Topical Similarity Score\n",
    "\n",
    "Latent Dirichlet Allocation references:\n",
    "[LDA scikit-learn official doc](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html);\n",
    "[LDA scikit-learn other](https://machinelearninggeek.com/latent-dirichlet-allocation-using-scikit-learn/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2091c6-99cc-49e7-91f3-500d82b889a4",
   "metadata": {},
   "source": [
    "Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "deaf69c7-2a1b-4e8a-b737-42cdd288c79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LDA_TRAIN_DOCS = 10_000\n",
    "N_TOPICS = [5, 10, 20, 30, 40, 50, 75, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08556fec-d5f8-4d52-8408-5ad1116afed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_rebel_path = Path('original')\n",
    "rebel_train_path = original_rebel_path.joinpath('en_train.jsonl')\n",
    "if not rebel_train_path.exists():\n",
    "    raise Exception(\n",
    "        f'Original REBEL dataset not existing! Please, download it and place it into the \"{original_rebel_path}\" folder.'\n",
    "        f' You can download it with the `rebel_download.sh` script or from \"https://huggingface.co/datasets/Babelscape/rebel-dataset/tree/main\".'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae67195-d3cd-48f8-ab2c-faedc3601a6c",
   "metadata": {},
   "source": [
    "Loading train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef30914f-528f-42e3-a01c-4228d9249d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of LDA train texts: 10,000\n"
     ]
    }
   ],
   "source": [
    "with rebel_train_path.open('r', encoding='utf-8') as f:\n",
    "    train_data = [json.loads(f.readline().strip()) for i in range(N_LDA_TRAIN_DOCS)]\n",
    "lda_train_texts = [td['text'] for td in train_data]\n",
    "print(f'Number of LDA train texts: {len(lda_train_texts):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0900afa9-dbce-4ef1-9abb-7fb556ddd24c",
   "metadata": {},
   "source": [
    "Processing train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34eb895e-b62b-4316-aa92-b0b99d0e19d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize regex tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68ab9db2-fd47-498f-a316-a5e03c6d7fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize document using TF-IDF\n",
    "tfidf = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words='english',\n",
    "    ngram_range = (1,1),\n",
    "    tokenizer = tokenizer.tokenize\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0be961dc-bde8-43b4-ace3-1d7411f7c2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Fit and transform the documents\n",
    "lda_train_texts_matrix = tfidf.fit_transform(lda_train_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8de3792-1ae9-4b3a-948c-8440ef2d3076",
   "metadata": {},
   "source": [
    "Modeling documents according to different numbers of latent topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0676e15a-0fcd-4633-905d-e4304ebd42f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 32.6 s\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda_models = dict()\n",
    "for n in N_TOPICS:\n",
    "    model = LatentDirichletAllocation(n_components=n)\n",
    "    model.fit(lda_train_texts_matrix)\n",
    "    lda_models[n] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3d11cc-8919-461f-a980-0988aa2d410d",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "\n",
    "*Running this evaluation again may result in slightly different Topic Similarity scores, as the trained LDA models may vary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cd6b3a9-65b0-4e2a-8772-b377cd370d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topical_sim(document_lda, triples_lda) -> float:\n",
    "    document_lda = document_lda.tolist()[0]\n",
    "    triples_lda = triples_lda.tolist()[0]\n",
    "    return math.e ** (-sum(document_lda[i] * math.log2(document_lda[i]/triples_lda[i]) for i in range(len(document_lda))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a9cd947-c5ce-4ddf-bc3e-35b385094192",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gpt_t_str(doc_t):\n",
    "    return f'{doc_t[\"subject label\"]}, {doc_t[\"predicate label\"]}, {doc_t[\"object label\"]}'\n",
    "\n",
    "def rebel_t_str(doc_t):\n",
    "    return f'{doc_t[\"subject\"][\"surfaceform\"]}, {doc_t[\"predicate\"][\"surfaceform\"]}, {doc_t[\"object\"][\"surfaceform\"]}'\n",
    "\n",
    "texts_l = list()\n",
    "triples_gpt_l = list()\n",
    "triples_rebel_l = list()\n",
    "for d in rebel_data:\n",
    "    texts_l.append(d['doc']['text'])\n",
    "    triples_gpt_l.append('\\t'.join([gpt_t_str(doc_t) for doc_t in d['triples']['gpt']]))\n",
    "    triples_rebel_l.append('\\t'.join([rebel_t_str(doc_t) for doc_t in d['triples']['gold']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "366c1ec8-f4ce-482b-b198-29b1e5c6e91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_l_matrix = tfidf.transform(texts_l)\n",
    "triples_gpt_l_matrix = tfidf.transform(triples_gpt_l)\n",
    "triples_rebel_l_matrix = tfidf.transform(triples_rebel_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb0c57e4-0264-413a-985a-d4eeee1f7f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "topical_sim_scores = dict()\n",
    "for n in N_TOPICS:\n",
    "    topical_sim_scores[n] = {'gpt': [], 'rebel': []}\n",
    "    for i in range(len(texts_l)):\n",
    "        topical_sim_scores[n]['gpt'].append(\n",
    "            topical_sim(lda_models[n].transform(texts_l_matrix[i]), lda_models[n].transform(triples_gpt_l_matrix[i]))\n",
    "        )\n",
    "        topical_sim_scores[n]['rebel'].append(\n",
    "            topical_sim(lda_models[n].transform(texts_l_matrix[i]), lda_models[n].transform(triples_rebel_l_matrix[i]))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7762fd12-c2f9-4635-b969-fbec49b1836f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOPICAL SIMILARITY SCORES\n",
      "N topics:    5 - Score GPT: 0.724 | Score REBEL: 0.648\n",
      "N topics:   10 - Score GPT: 0.704 | Score REBEL: 0.682\n",
      "N topics:   20 - Score GPT: 0.568 | Score REBEL: 0.469\n",
      "N topics:   30 - Score GPT: 0.510 | Score REBEL: 0.435\n",
      "N topics:   40 - Score GPT: 0.406 | Score REBEL: 0.319\n",
      "N topics:   50 - Score GPT: 0.413 | Score REBEL: 0.323\n",
      "N topics:   75 - Score GPT: 0.311 | Score REBEL: 0.200\n",
      "N topics:  100 - Score GPT: 0.241 | Score REBEL: 0.148\n"
     ]
    }
   ],
   "source": [
    "print('TOPICAL SIMILARITY SCORES')\n",
    "for n in N_TOPICS:\n",
    "    print(f'N topics: {n:4} - Score GPT: {np.average(topical_sim_scores[n][\"gpt\"]):4.3f} | Score REBEL: {np.average(topical_sim_scores[n][\"rebel\"]):4.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190c8720-4d8b-427a-90b8-98893453ca23",
   "metadata": {},
   "source": [
    "## Uniqueness Score\n",
    "\n",
    "Embeddings references: [Sentence Embeddings](https://www.sbert.net/); [Models](https://www.sbert.net/docs/pretrained_models.html#model-overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c651e6-f217-485e-925e-4514fd17ce98",
   "metadata": {},
   "source": [
    "Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19a7ecba-c92a-4059-91d3-bb5a90141145",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLDS = [v / 1000 for v in range(700, 1000, 50)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6293133-853b-453e-a046-43eb32d8e142",
   "metadata": {},
   "source": [
    "Loading embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5eb66fd-4e8b-400c-ab4d-f4e144791351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 250 ms\n",
      "Wall time: 4.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture\n",
    "embedding_model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ac39d8-c791-4eeb-8eab-b3dcd33d2199",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf6b211f-3a9e-4dff-99af-b28f65c2c431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(v1, v2) -> float:\n",
    "    return cosine_similarity([v1], [v2])[0][0]\n",
    "\n",
    "def uniqueness(triples_list: list, threshold: float) -> float:\n",
    "    assert all([isinstance(t, list) and len(t) == 3 for t in triples_list])\n",
    "    n = len(triples_list)\n",
    "    v = [embedding_model.encode(', '.join(t)) for t in triples_list]\n",
    "    return sum([int(cos_sim(v[i], v[j]) < threshold) for i in range(n) for j in range(n) if i != j]) / (n * (n-1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1afa48fe-7be8-4c74-a015-7a59618e5cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_t_list(doc_t):\n",
    "    labels = ['subject label', 'predicate label', 'object label']\n",
    "    return [doc_t[l] for l in labels]\n",
    "\n",
    "def rebel_t_list(doc_t):\n",
    "    labels = ['subject', 'predicate', 'object']\n",
    "    return [doc_t[l]['surfaceform'] for l in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c62adbae-6826-41df-ac1d-da86d5bafcd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 6min 43s\n",
      "Wall time: 12min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Not the smartest way of computing it, as encoding is repeated multiple times ...\n",
    "uniqueness_scores = dict()\n",
    "for th in THRESHOLDS:\n",
    "    uniqueness_scores[th] = {'gpt': [], 'rebel': []}\n",
    "    for i in range(len(texts_l)):\n",
    "        gpt_triples = [gpt_t_list(doc_t) for doc_t in d['triples']['gpt']]\n",
    "        uniqueness_scores[th]['gpt'].append(uniqueness(gpt_triples, th))\n",
    "        \n",
    "        rebel_triples = [rebel_t_list(doc_t) for doc_t in d['triples']['gold']]\n",
    "        uniqueness_scores[th]['rebel'].append(uniqueness(rebel_triples, th))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a333ace3-df53-4910-bb04-08b51d07e49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIQUENESS SCORES\n",
      "Similarity threshold: 0.70 - Score GPT: 0.924 | Score REBEL: 1.000\n",
      "Similarity threshold: 0.75 - Score GPT: 0.952 | Score REBEL: 1.000\n",
      "Similarity threshold: 0.80 - Score GPT: 0.981 | Score REBEL: 1.000\n",
      "Similarity threshold: 0.85 - Score GPT: 0.990 | Score REBEL: 1.000\n",
      "Similarity threshold: 0.90 - Score GPT: 0.995 | Score REBEL: 1.000\n",
      "Similarity threshold: 0.95 - Score GPT: 1.000 | Score REBEL: 1.000\n"
     ]
    }
   ],
   "source": [
    "print('UNIQUENESS SCORES')\n",
    "for th in THRESHOLDS:\n",
    "    print(f'Similarity threshold: {th:3.2f} - Score GPT: {np.average(uniqueness_scores[th][\"gpt\"]):4.3f} | Score REBEL: {np.average(uniqueness_scores[th][\"rebel\"]):4.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca33c7d-b84a-4bc0-b84b-98ad836f5012",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
